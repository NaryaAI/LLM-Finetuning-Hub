{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8a9dce",
   "metadata": {},
   "source": [
    "# TRAIN\n",
    "This script is an example of how you can fine-tune **HuggingFaceH4/zephyr-7b-alpha** on **Narya-ai/title_subtitle** dataset \\\n",
    "* Use peft and quantization to fit the model for training on the smaller GPU instance \\\n",
    "* Save the checkpoints and the pefted model \\ \n",
    "* Push to Huggingface hub \\\n",
    "\n",
    "\n",
    "For Inference see **zephyr_inference_conda_pytorch_310.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2c847f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.0.1)\n",
      "Collecting transformers[torch]\n",
      "  Obtaining dependency information for transformers[torch] from https://files.pythonhosted.org/packages/c1/bd/f64d67df4d3b05a460f281defe830ffab6d7940b7ca98ec085e94e024781/transformers-4.34.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/7c/55/b3432f43d6d7fee999bb23a547820d74c48ec540f5f7842e41aa5d8d5f3a/datasets-2.14.6-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[torch]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[torch]) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers[torch])\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers[torch])\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers[torch])\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[torch]) (4.65.0)\n",
      "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
      "  Obtaining dependency information for accelerate>=0.20.3 from https://files.pythonhosted.org/packages/13/9e/ee987874058f2d93006961f6ff49e0bcb60ab9c26709ebe06bfa8707a4d8/accelerate-0.24.1-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/80/8a/1dd41557883b6196f8f092011a5c1f72d4d44cf36d7b67d4a5efe3127949/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/41/8e/4c48881316bbced3d13089c4d0df4be321ce79a0c695d82dee9996aaf56b/aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Installing collected packages: xxhash, safetensors, regex, multidict, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, tokenizers, aiohttp, accelerate, transformers, datasets\n",
      "Successfully installed accelerate-0.24.1 aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.6 frozenlist-1.4.0 huggingface-hub-0.17.3 multidict-6.0.4 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1 xxhash-3.4.1 yarl-1.9.2\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Collecting peft\n",
      "  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\n",
      "  Downloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (4.34.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (0.24.1)\n",
      "Requirement already satisfied: safetensors in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft) (0.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate->peft) (0.17.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->peft) (0.14.1)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub->accelerate->peft) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers->peft) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.5.0\n",
      "Collecting trl\n",
      "  Obtaining dependency information for trl from https://files.pythonhosted.org/packages/ce/b2/b0e9c7a15d666aebe83ed72b6a3bec869be88246ddf22d8953f3eee61e22/trl-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading trl-0.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from trl) (2.0.1)\n",
      "Requirement already satisfied: transformers>=4.18.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from trl) (4.34.1)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from trl) (1.24.4)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from trl) (0.24.1)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from trl) (2.14.6)\n",
      "Collecting tyro>=0.5.7 (from trl)\n",
      "  Obtaining dependency information for tyro>=0.5.7 from https://files.pythonhosted.org/packages/f0/ee/72cb2647dc72ef6412e5cbca8bbbae7f757c872ca047e421a2d78d7b890f/tyro-0.5.12-py3-none-any.whl.metadata\n",
      "  Downloading tyro-0.5.12-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.18.0->trl) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.18.0->trl) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.18.0->trl) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.18.0->trl) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.18.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.18.0->trl) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.18.0->trl) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.18.0->trl) (4.65.0)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.7->trl)\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.7->trl)\n",
      "  Obtaining dependency information for rich>=11.1.0 from https://files.pythonhosted.org/packages/be/2a/4e62ff633612f746f88618852a626bbe24226eba5e7ac90e91dcfd6a414e/rich-13.6.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.7->trl)\n",
      "  Obtaining dependency information for shtab>=1.5.6 from https://files.pythonhosted.org/packages/86/69/3a4873b36d65a1b8f4ee606f5a785b5babb9960385802de60d8455e2f8b6/shtab-1.6.4-py3-none-any.whl.metadata\n",
      "  Downloading shtab-1.6.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate->trl) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->trl) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->trl) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->trl) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->trl) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->trl) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->trl) (3.8.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.18.0->trl) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl) (2023.5.7)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.7->trl)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.7->trl) (2.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.7->trl)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->trl) (1.16.0)\n",
      "Downloading trl-0.7.2-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.0/124.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.5.12-py3-none-any.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.6.0-py3-none-any.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.8/239.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.6.4-py3-none-any.whl (13 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shtab, mdurl, docstring-parser, markdown-it-py, rich, tyro, trl\n",
      "Successfully installed docstring-parser-0.15 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.6.0 shtab-1.6.4 trl-0.7.2 tyro-0.5.12\n",
      "Collecting bitsandbytes\n",
      "  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/1e/2c/af22cd797fc368a9f098ed03015730e6568b884fe67f9940793d944a4b7b/bitsandbytes-0.41.1-py3-none-any.whl.metadata\n",
      "  Downloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.41.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch transformers[torch] datasets\n",
    "# !pip install accelerate -U\n",
    "# !pip install peft\n",
    "# !pip install trl\n",
    "# !pip install bitsandbytes\n",
    "# !pip install ninja packaging\n",
    "# !MAX_JOBS=32 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d69fce95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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\n",
    "# !huggingface-cli login --token <TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d51fee5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44740"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, pipeline, AutoModel, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d307b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load original Zephyr model and Zephyr tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bd9e33f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7fb636831f4486968f1318b08edb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens': 0,\n",
       " 'model.layers.0': 0,\n",
       " 'model.layers.1': 0,\n",
       " 'model.layers.2': 0,\n",
       " 'model.layers.3': 1,\n",
       " 'model.layers.4': 1,\n",
       " 'model.layers.5': 1,\n",
       " 'model.layers.6': 1,\n",
       " 'model.layers.7': 2,\n",
       " 'model.layers.8': 2,\n",
       " 'model.layers.9': 2,\n",
       " 'model.layers.10': 2,\n",
       " 'model.layers.11': 3,\n",
       " 'model.layers.12': 3,\n",
       " 'model.layers.13': 3,\n",
       " 'model.layers.14': 3,\n",
       " 'model.layers.15': 4,\n",
       " 'model.layers.16': 4,\n",
       " 'model.layers.17': 4,\n",
       " 'model.layers.18': 4,\n",
       " 'model.layers.19': 5,\n",
       " 'model.layers.20': 5,\n",
       " 'model.layers.21': 5,\n",
       " 'model.layers.22': 5,\n",
       " 'model.layers.23': 6,\n",
       " 'model.layers.24': 6,\n",
       " 'model.layers.25': 6,\n",
       " 'model.layers.26': 6,\n",
       " 'model.layers.27': 7,\n",
       " 'model.layers.28': 7,\n",
       " 'model.layers.29': 7,\n",
       " 'model.layers.30': 7,\n",
       " 'model.layers.31': 7,\n",
       " 'model.norm': 7,\n",
       " 'lm_head': 7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ",model_name = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    ),\n",
    "    device_map=\"auto\",\n",
    "    # IMPORTANT, FA2 gives 3+x speed up\n",
    "    # https://github.com/Dao-AILab/flash-attention\n",
    "    # https://github.com/huggingface/peft/issues/790#issuecomment-1696801352\n",
    "    use_flash_attention_2=True,\n",
    ")\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# IMPORTANT IF ON SAGEMAKER NOTEBOOK. set path to folder that is on a LARGE disk\n",
    "# otherwise disk get's full fast and then gives \"No space left error\"\n",
    "# can check the disks spaces with \"df -h\" on terminal\n",
    "cache_dir = \"/home/ec2-user/SageMaker/cache\"\n",
    "\n",
    "# if all good the model weights must be distributed across all available gpus\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d55f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load **Narya-ai/title_subtitle** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d464e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2293\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_data(d):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": d['system_message']},\n",
    "        {\"role\": \"user\", \"content\": d['user_message']},\n",
    "        {\"role\": \"assistant\", \"content\": d['completion']},\n",
    "    ]\n",
    "    return {'text': tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True)}\n",
    "\n",
    "def tokenize_function(d):\n",
    "    source_encodings = tokenizer(d['text'], return_tensors=\"pt\",\n",
    "                                 padding='max_length', truncation=True, max_length=2500)\n",
    "    source_encodings[\"labels\"] = source_encodings[\"input_ids\"]\n",
    "    return source_encodings\n",
    "\n",
    "dataset_path = 'title_subtitle_dataset.jsonl'\n",
    "dataset = load_dataset('json', data_files={'train': dataset_path}, split='train')\n",
    "dataset = dataset.map(convert_data, remove_columns=['system_message', 'user_message', 'completion'])\n",
    "dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2).values()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7359880",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b901035c544c1694e2bd82ebeaf90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1834 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1142b4c36b458e8b4868cd7ff65412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # IMPORTANT Adjust based on your GPU's capabilities\n",
    "    per_device_train_batch_size=1,  \n",
    "    per_device_eval_batch_size=1,\n",
    "    # IMPORTANT local folder to save checkpoints. if call trainer.save_model saves to this folder\n",
    "    # trainer.push_to_hub uploads from this folder\n",
    "    output_dir=\"./some_output\", \n",
    "    # IMPORTANT if you use trainer.push_to_hub it uploads to this repo-id\n",
    "    hub_model_id=\"Narya-ai/zephyr-title-subtitle\",\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,  # Set to True if you want to push to HuggingFace's Model Hub\n",
    "    fp16=True,  # Use mixed precision for faster training (ensure you have the `apex` library installed)\n",
    "    gradient_accumulation_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    hub_private_repo=True,\n",
    ")\n",
    "\n",
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2500,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94303deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b4957",
   "metadata": {},
   "source": [
    "## There are 2 ways to save and upload to hub the fine-tuned model\n",
    "### 1. Via Trainer(recommended)\n",
    "\n",
    "\n",
    "Note:\n",
    "* uploads only necessary files including adapters, tokenizer, etc.(NOT THE MODEL ITSELF)\n",
    "* It uploads the files saved in output_dir that you set in TrainingArguments\n",
    "* It uses the hub_model_id (user_name/model_name) that you set in TrainingArguments. The default is [here](https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer#transformers.TrainingArguments.hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "402f1160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d46eb2d8a949dfa7b054c00e2a1090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79341f638269489c87a58fe1758e9158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd20051f1484cea84d57c4810e62f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/680M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Narya-ai/zephyr-title-subtitle/tree/main/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd22235",
   "metadata": {},
   "source": [
    "# If you wanted to only fine-tune and upload/save model STOP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45950b",
   "metadata": {},
   "source": [
    "# This is just to understand what is happening to the model and what is being uploaded by different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488023c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_weight = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"model: {s_weight} after training\")\n",
    "\n",
    "# for name in [\"HuggingFaceH4/zephyr-7b-alpha\", \"Narya-ai/zephyr-title-subtitle\", \"Narya-ai/zephyr-title-subtitle-full\"]:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         name,\n",
    "#         load_in_8bit=True,\n",
    "#         device_map=\"auto\",\n",
    "#         cache_dir=cache_dir,\n",
    "#     )\n",
    "#     s_weight = sum(p.numel() for p in model.parameters())\n",
    "#     print(f\"{name}: {s_weight}\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "#     load_in_8bit=True,\n",
    "#     device_map=\"auto\",\n",
    "#     cache_dir=cache_dir,\n",
    "# )\n",
    "# adapter_name = model.load_adapter(\"Narya-ai/zephyr-title-subtitle\")\n",
    "# model.active_adapters = adapter_name\n",
    "# s_weight = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"merging with original: {s_weight}\")\n",
    "\n",
    "\n",
    "# s_weight = sum(p.numel() for p in trainer.model.parameters())\n",
    "# print(f\"trainer.model: {s_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be999787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to give a sense of what is happening with the weights\n",
    "# Note the numbers might change after each training, but the pattern must stay the same\n",
    "# summing weights using sum(p.numel() for p in model.parameters()) on models:\n",
    "\n",
    "\"\"\"\n",
    "ORIGINAL MODEL - NOT FINE-TUNED\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "7241732096\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ONLY ADAPTERS\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Narya-ai/zephyr-title-subtitle\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "7284252672\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ONLY THE MODEL\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Narya-ai/zephyr-title-subtitle-full\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "7241732096\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Merging adapters to the original function\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "adapter_name = model.load_adapter(\"Narya-ai/zephyr-title-subtitle\")\n",
    "model.active_adapters = adapter_name\n",
    "7284252672\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a85de",
   "metadata": {},
   "source": [
    "### 2. Via model.push_to_hub \n",
    "Note:\n",
    "* To upload the fine-tuned model(not original) you need to merge adapters first\n",
    "* This function will only push the ORIGINAL model, you'll need to push tokenizer too\n",
    "* This will upload the whole model, so it will be heavier memory wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b877160",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/home/ec2-user/SageMaker/cache\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"cpu\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "adapter = PeftModel.from_pretrained(\n",
    "    model=model,\n",
    "    model_id=\"./some_output/checkpoint-200\",\n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"cpu\",\n",
    "    \n",
    ")\n",
    "model = adapter.merge_and_unload(progressbar=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.save_pretrained(\n",
    "    \"Narya-ai/zephyr-title-subtitle\",\n",
    "    push_to_hub=True,\n",
    "    repo_id=\"Narya-ai/zephyr-title-subtitle\",\n",
    "    private=True,\n",
    "    max_shard_size=\"4GB\"\n",
    "\n",
    ")\n",
    "tokenizer.save_pretrained(\n",
    "    \"Narya-ai/zephyr-title-subtitle\",\n",
    "    push_to_hub=True,\n",
    "    repo_id=\"Narya-ai/zephyr-title-subtitle\",\n",
    "    private=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a2885",
   "metadata": {},
   "source": [
    "### Quick inference check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(\"text-generation\", model=\"Narya-ai/zephyr-title-subtitle\", \n",
    "#                 torch_dtype=torch.bfloat16, device_map=\"auto\", tokenizer=tokenizer)\n",
    "\n",
    "# prompt = '<|system|>\\n\\nIn this task you need to identify TITLE and SUBTITLE of a webpage that contains a web article.\\nGuidelines:\\n- TITLE is always present while SUBTITLE can be missing often\\n- TITLE describes the main story of the article, while SUBTITLE usually is akin to an explanation of clarification of what the article is about, sometimes giving motivation to the story\\n- SUBTITLE cannot be a list of author names, or publication date\\n- SUBTITLE does not begin with words \"abstract\" or \"summary\"\\n- SUBTITLE  cannot be larger than around 3 sentences\\n- Research papers should only have TITLE, no SUBTITLE\\n- If you see an obvious error in TITLE or SUBTITLE wording fix it\\n\\nYou have the following inputs:\\n1) SUMMARY - a summary of the article so you understand what its about. Use it to check whether resulting TITLE and SUBTITLE make sense.\\n2) SOURCE_URL - the source url for the webpage, sometimes using that you can understand roughly what the article main description is.\\n3) SUGGESTED_TITLE - this is the prediction from an html-based extractor. A lot of the times its correct but sometimes its wrong\\n4) SUGGESTED_SUBTITLE - this is the prediction from an html-based extractor. This is fairly often wrong\\n5) FONTHTML - the list of html-derived text boxes with corresponding positions and fonts. It looks like a list of tuples, each tuple format is like this:\\n(\\'Some text\\', \\'14px\\', \\'400\\', \\'24px\\', 40, 9696.625, 1120, 24)\\nHere first item is text, then we have fontSize, fontWeight, lineHeight, x, y, width and height. Use this information to identify likely pieces of TITLE/SUBTITLE as they appear on top of the image, and sometimes are larger fonts\\n\\nThe correct guess for SUBTITLE should always follow the TITLE in the page. If in between there is any info - author information, date, image captions etc - then its not SUBTITLE.\\n\\nSUGGESTED_TITLE  is an ok option for best guess for TITLE when the OCR_text contains only rubbish, but please make sure it follows the guidelines above.\\n\\nPlease return TITLE and SUBTITLE only as a structured dictionary with two keys: \"TITLE\" and \"SUBTITLE\"\\n</s>\\n<|user|>\\n\\nSUMMARY:\\nLena anderson, a fictional personality played by artificial intelligence (AI) software powering chatbots like OpenAIasOpenaiasGpt, discusses soccer with three real-world bot characters from fantasy. They discuss ways to reach new audiences for Major League Soccer (Mls), including augmented reality features, agamificationa, and the use of natural language models in an augmented reality mobile app. The bots, developed by fantasy\\'s synthetic humans, can be used to test product concepts, generate new ideas, and help clients develop products. Another bot, trained on openAIas gpt - 4 language model, uses machine learning technology that powers chatbots such as openaias Gpt and GoogleasBard. It also builds chatbots based on data from diverse populations. A third bot simulates market behavior using openAIAS gpt4s\\' neural networks. This is similar to the stanford university\\'s genetic engineering research, which led to the creation of \\'Synthetic Humans\\', but they do not have access to existing product lines or knowledge of specific product lines. Despite these efforts, however, there are concerns about how to make this approach more effective. These discussions highlight the need for companies to invest in R&D to improve their AI capabilities, especially due to the growth of online gaming. Lena anderson suggests investing in projects like AR/VR, targeted advertising, ecommerce, and social media marketing, and blockchain funding, while other companies may offer additional funding.\\nSOURCE_URL:\\nhttps://www.wired.com/story/fast-forward-the-chatbots-are-now-talking-to-each-other/\\nSUGGESTED_TITLE:\\nThe Chatbots Are Now Talking to Each Other\\nSUGGESTED_SUBTITLE:\\nThe Chatbots Are Now Talking to Each Other\\nFONT_HTML:\\n[(\\'Skip to main content\\', \\'19px\\', \\'400\\', \\'28px\\', 0, 56, 1, 1), (\\'Will Knight\\', \\'11px\\', \\'400\\', \\'12.98px\\', 56, 112, 69.890625, 12), (\\'Business\\', \\'11px\\', \\'400\\', \\'11px\\', 151.890625, 112, 64.4375, 12), (\\'Oct 12, 2023 11:00 AM\\', \\'11px\\', \\'400\\', \\'12.98px\\', 232.328125, 104, 128.015625, 20.96875), (\\'The Chatbots Are Now Talking to Each Other\\', \\'46.8px\\', \\'700\\', \\'46.8px\\', 48, 166.96875, 1104, 62.796875), (\\'ChatGPT-style chatbots that pretend to be people are being used to help companies develop new product and marketing ideas.\\', \\'20px\\', \\'700\\', \\'24px\\', 48, 229.765625, 1104, 48), (\\'Photograph: Carol Yepes/Getty Images\\', \\'11px\\', \\'400\\', \\'12.98px\\', 48, 894.75, 264.375, 12), (\\'Lena Anderson isn’t a soccer fan, but she does spend a lot of time ferrying her kids between soccer practices and competitive games.\\', \\'19px\\', \\'400\\', \\'28px\\', 137.328125, 973.703125, 504, 84), (\\'“I may not pull out a foam finger and painted face, but soccer does have a place in my life,” says the soccer mom—who also happens to be completely made up. Anderson is a fictional personality played by artificial intelligence software like that powering ChatGPT.\\', \\'19px\\', \\'400\\', \\'28px\\', 137.328125, 1076.703125, 504, 140), (\\'Content\\', \\'19px\\', \\'700\\', \\'28px\\', 137.328125, 1235.703125, 504, 28), (\\'To honor your privacy preferences, this content can only be viewed on the site it originates from.\\', \\'12px\\', \\'400\\', \\'16px\\', 137.328125, 1279.703125, 504, 32), (\\'Sign Up Today\\', \\'28px\\', \\'700\\', \\'28px\\', 137.328125, 1338.703125, 188.828125, 31), (\"This is an edition of WIRED\\'s Fast Forward newsletter, a weekly dispatch from the future by Will Knight, exploring AI advances and other technology set to change our lives.\", \\'16px\\', \\'700\\', \\'18px\\', 137.328125, 1638.6875, 270, 108), (\\'Anderson doesn’t let her imaginary status get in the way of her opinions, though, and comes complete with a detailed backstory. In a wide-ranging conversation with a human interlocutor, the bot says that it has a 7-year-old son who is a fan of the New England Revolution and loves going to home games at Gillette Stadium in Massachusetts. Anderson claims to think the sport is a wonderful way for kids to stay active and make new friends.\\', \\'19px\\', \\'400\\', \\'28px\\', 137.328125, 1334.703125, 504, 504), (\\'In another conversation, two more AI characters, Jason Smith and Ashley Thompson, talk to one another about ways that Major League Soccer (MLS) might reach new audiences. Smith suggests a mobile app with an augmented reality feature showing different views of games. Thompson adds that the app could include “gamification” that lets players earn points as they watch.\\', \\'19px\\', \\'400\\', \\'28px\\', 137.328125, 1857.703125, 504, 196), (\\'The three bots are among scores of AI characters that have been developed by Fantasy, a New York company that helps businesses such as LG, Ford, Spotify, and Google dream up and test new product ideas. Fantasy calls its bots synthetic humans and says they can help clients learn about audiences, think through product concepts, and even generate new ideas, like the soccer app.\\', \\'19px\\', \\'400\\', \\'28px\\', 137.328125, 2072.703125, 504, 224), (\\'\"The technology is truly incredible,\" says Cole Sletten, VP of digital experience at the MLS. “We’re already seeing huge value and this is just the beginning.”\\', \\'19px\\', \\'400\\', \\'28px\\', 137.328125, 2315.703125, 504, 84), (\\'Video: Fantasy\\', \\'11px\\', \\'400\\', \\'12.98px\\', 137.328125, 2455.703125, 96.59375, 12), (\\'Fantasy uses the kind of machine learning technology that powers chatbots like OpenAI’s ChatGPT and Google’s Bard to create its synthetic humans. The company gives each agent dozens of characteristics drawn from ethnographic research on real people, feeding them into commercial large language models like OpenAI’s GPT and Anthropic’s Claude. Its agents can also be set up to have knowledge of existing product lines or businesses, so they can converse about a client’s offerings.\\', \\'19px\\', \\'400\\', \\'28px\\', 137.328125, 2529.65625, 504, 252), (\\'Video: Fantasy\\', \\'11px\\', \\'400\\', \\'12.98px\\', 137.328125, 2837.65625, 96.59375, 12), (\"Fantasy then creates focus groups of both synthetic humans and real people. The participants are given a topic or a product idea to discuss, and Fantasy and its client watch the chatter. BP, an oil and gas company, asked a swarm of 50 of Fantasy’s synthetic humans to discuss ideas for smart city projects. “We\\'ve gotten a really good trove of ideas,” says Roger Rohatgi, BP’s global head of design. “Whereas a human may get tired of answering questions or not want to answer that many ways, a synthetic human can keep going,” he says.\", \\'19px\\', \\'400\\', \\'28px\\', 137.328125, 2911.609375, 504, 280), (\\'Peter Smart, chief experience officer at Fantasy, says that synthetic humans have produced novel ideas for clients, and prompted real humans included in their conversations to be more creative. “It is fascinating to see novelty—genuine novelty—come out of both sides of that equation—it’s incredibly interesting,” he says.\\', \\'19px\\', \\'400\\', \\'28px\\', 137.3281\\nTITLE AND SUBTITLE:</s>\\n<|assistant|>\\n'\n",
    "# outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.1, top_k=50, top_p=0.95)\n",
    "# print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
